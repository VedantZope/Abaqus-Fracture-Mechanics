{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a dummy Bayesian optimization algorithm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern # you can try to import other kernels from sklearn as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare target flow curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('abaqus simulation F-D fitting/Flowcurve_RT.csv')\n",
    "# print(df)\n",
    "# Extract the true strain and average true stress columns\n",
    "trueStrain = df['True strain']\n",
    "trueStress = df['Avg.True stress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous searching space\n",
    "\n",
    "param_bounds = {\n",
    "    \"c1\": (700, 1800),  \n",
    "    \"c2\": (0.1 * 1e2, 10 * 1e2) ,    \n",
    "    \"c3\": (0.01  * 1e3, 0.1 * 1e3)\n",
    "}\n",
    "\n",
    "# Swift Laws\n",
    "\n",
    "def swift_law(c1,c2,c3,strain):\n",
    "    c2_temp=c2*1e-14 * 1e-2\n",
    "    c3_temp=c3* 1e-3\n",
    "    return c1* np.exp(c3_temp * np.log(c2_temp + strain))\n",
    "\n",
    "# Note: BO in Bayes-Opt tries to maximize, so you should use the inverse of the loss function.\n",
    "\n",
    "def lossFunction(**solution):\n",
    "    #print(solution)\n",
    "    c1 = solution[\"c1\"]\n",
    "    c2 = solution[\"c2\"]\n",
    "    c3 = solution[\"c3\"]\n",
    "    simStress = swift_law(c1,c2,c3,trueStrain)\n",
    "\n",
    "    # RMSE loss\n",
    "    fitness = np.sqrt(np.mean((simStress - trueStress)**2))\n",
    "    loss = -fitness\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BO():\n",
    "    \n",
    "    ##################################\n",
    "    # OPTIMIZER CLASS INITIALIZATION #\n",
    "    ##################################\n",
    "\n",
    "    def __init__(self):        \n",
    "        #############################\n",
    "        # Optimizer hyperparameters #\n",
    "        #############################\n",
    "        \n",
    "        # maximize parameters\n",
    "        self.verbose = 1 # 0 for no output, 1 for some output printing\n",
    "        self.random_state = 123 # random seed\n",
    "        self.init_points = 0 # number of initial points to sample randomly for Bayesian optimization\n",
    "        self.iterations = 1 # number of iterations to run Bayesian optimization\n",
    "        \n",
    "        # Acquisition function        \n",
    "        # Low kappa means more exploitation for UCB\n",
    "        # High kappa means more exploration for UCB\n",
    "        # Low xi means more exploitation for EI and POI\n",
    "        # High xi means more exploration for EI and POI\n",
    "        self.acquisitionFunction = UtilityFunction(kind='ucb', kappa=2.576, xi=0, kappa_decay=1, kappa_decay_delay=0)\n",
    "        #self.acquisitionFunction = UtilityFunction(kind='poi', kappa=2.576, xi=0, kappa_decay=1, kappa_decay_delay=0)\n",
    "        #self.acquisitionFunction = UtilityFunction(kind='ei', kappa=2.576, xi=0, kappa_decay=1, kappa_decay_delay=0)\n",
    "        \n",
    "        # Gaussian process kernel parameters\n",
    "        #self.GP_kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3)) # RBF kernel\n",
    "        self.GP_kernel = Matern(nu=2.5) # Matern kernel\n",
    "        self.alpha = 1e-6\n",
    "        self.normalize_y=True\n",
    "        self.n_restarts_optimizer=5\n",
    "        self.logger = JSONLogger(path=\"bayesopt_log/logs.log\", reset=False)\n",
    "        \n",
    "    ##########################\n",
    "    # OPTIMIZATION FUNCTIONS #\n",
    "    ##########################\n",
    "\n",
    "    def initializeOptimizer(self, lossFunction, param_bounds, loadingProgress = False):\n",
    "        self.param_bounds = param_bounds\n",
    "        self.lossFunction = lossFunction\n",
    "        self.loadingProgress = loadingProgress\n",
    "        BO_bounds = param_bounds\n",
    "        bo_instance = BayesianOptimization(\n",
    "            f = lossFunction,\n",
    "            pbounds = BO_bounds, \n",
    "            verbose = self.verbose,\n",
    "            random_state = self.random_state,\n",
    "            bounds_transformer = None,\n",
    "            allow_duplicate_points = False,\n",
    "        )\n",
    "        bo_instance.set_gp_params(\n",
    "            kernel=self.GP_kernel,\n",
    "            alpha=self.alpha,\n",
    "            normalize_y=self.normalize_y,\n",
    "            n_restarts_optimizer=self.n_restarts_optimizer,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        self.optimizer = bo_instance\n",
    "        if loadingProgress == False:\n",
    "            self.optimizer.subscribe(Events.OPTIMIZATION_STEP, self.logger)\n",
    "        else:\n",
    "            if os.path.exists(\"./bayesopt_log/logs.log.json\"):\n",
    "                #self.optimizer.subscribe(Events.OPTIMIZATION_STEP, self.logger)\n",
    "                load_logs(self.optimizer, logs=[\"./bayesopt_log/logs.log.json\"]);\n",
    "                print(\"New optimizer is now aware of {} points.\".format(len(self.optimizer.space)))\n",
    "                self.optimizer.subscribe(Events.OPTIMIZATION_STEP, self.logger)\n",
    "            else:\n",
    "                print(\"hello\")\n",
    "            \n",
    "\n",
    "    def run(self):\n",
    "        self.optimizer.maximize(\n",
    "            init_points = self.init_points if self.loadingProgress == False else 0, \n",
    "            n_iter = self.iterations,   \n",
    "            acquisition_function=self.acquisitionFunction\n",
    "        )\n",
    "        \n",
    "    def outputResult(self):\n",
    "        solution_dict = self.optimizer.max[\"params\"]\n",
    "        solution_tuple = tuple(solution_dict.items())\n",
    "        best_solution_loss = self.optimizer.max[\"target\"]\n",
    "        return solution_dict, solution_tuple, best_solution_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGaussianProcessRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:305\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_queue)\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:27\u001b[0m, in \u001b[0;36mQueue.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mempty:\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mQueue is empty, no more objects to retrieve.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mStopIteration\u001b[0m: Queue is empty, no more objects to retrieve.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m BO_instance \u001b[39m=\u001b[39m BO()\n\u001b[0;32m      2\u001b[0m BO_instance\u001b[39m.\u001b[39minitializeOptimizer(lossFunction, param_bounds, loadingProgress\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m BO_instance\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m      4\u001b[0m solution_dict, solution_tuple, best_solution_loss \u001b[39m=\u001b[39m BO_instance\u001b[39m.\u001b[39moutputResult()\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(solution_dict)\n",
      "Cell \u001b[1;32mIn[8], line 73\u001b[0m, in \u001b[0;36mBO.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mmaximize(\n\u001b[0;32m     74\u001b[0m         init_points \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_points \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloadingProgress \u001b[39m==\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m, \n\u001b[0;32m     75\u001b[0m         n_iter \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterations,   \n\u001b[0;32m     76\u001b[0m         acquisition_function\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49macquisitionFunction\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:308\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     util\u001b[39m.\u001b[39mupdate_params()\n\u001b[1;32m--> 308\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuggest(util)\n\u001b[0;32m    309\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobe(x_probe, lazy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:226\u001b[0m, in \u001b[0;36mBayesianOptimization.suggest\u001b[1;34m(self, utility_function)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstraint\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space\u001b[39m.\u001b[39mparams,\n\u001b[0;32m    223\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space\u001b[39m.\u001b[39m_constraint_values)\n\u001b[0;32m    225\u001b[0m \u001b[39m# Finding argmax of the acquisition function.\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m suggestion \u001b[39m=\u001b[39m acq_max(ac\u001b[39m=\u001b[39;49mutility_function\u001b[39m.\u001b[39;49mutility,\n\u001b[0;32m    227\u001b[0m                      gp\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gp,\n\u001b[0;32m    228\u001b[0m                      constraint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstraint,\n\u001b[0;32m    229\u001b[0m                      y_max\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_space\u001b[39m.\u001b[39;49mtarget\u001b[39m.\u001b[39;49mmax(),\n\u001b[0;32m    230\u001b[0m                      bounds\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_space\u001b[39m.\u001b[39;49mbounds,\n\u001b[0;32m    231\u001b[0m                      random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_random_state)\n\u001b[0;32m    233\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space\u001b[39m.\u001b[39marray_to_params(suggestion)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\util.py:78\u001b[0m, in \u001b[0;36macq_max\u001b[1;34m(ac, gp, y_max, bounds, random_state, constraint, n_warmup, n_iter)\u001b[0m\n\u001b[0;32m     74\u001b[0m     to_minimize \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39m-\u001b[39mac(x\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), gp\u001b[39m=\u001b[39mgp, y_max\u001b[39m=\u001b[39my_max)\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m x_try \u001b[39min\u001b[39;00m x_seeds:\n\u001b[0;32m     77\u001b[0m     \u001b[39m# Find the minimum of minus the acquisition function\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     res \u001b[39m=\u001b[39m minimize(\u001b[39mlambda\u001b[39;49;00m x: to_minimize(x),\n\u001b[0;32m     79\u001b[0m                    x_try,\n\u001b[0;32m     80\u001b[0m                    bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[0;32m     81\u001b[0m                    method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     83\u001b[0m     \u001b[39m# See if success\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m res\u001b[39m.\u001b[39msuccess:\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\util.py:78\u001b[0m, in \u001b[0;36macq_max.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     74\u001b[0m     to_minimize \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39m-\u001b[39mac(x\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), gp\u001b[39m=\u001b[39mgp, y_max\u001b[39m=\u001b[39my_max)\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m x_try \u001b[39min\u001b[39;00m x_seeds:\n\u001b[0;32m     77\u001b[0m     \u001b[39m# Find the minimum of minus the acquisition function\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     res \u001b[39m=\u001b[39m minimize(\u001b[39mlambda\u001b[39;00m x: to_minimize(x),\n\u001b[0;32m     79\u001b[0m                    x_try,\n\u001b[0;32m     80\u001b[0m                    bounds\u001b[39m=\u001b[39mbounds,\n\u001b[0;32m     81\u001b[0m                    method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL-BFGS-B\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m     \u001b[39m# See if success\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m res\u001b[39m.\u001b[39msuccess:\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\util.py:74\u001b[0m, in \u001b[0;36macq_max.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[39mreturn\u001b[39;00m target \u001b[39m/\u001b[39m (\u001b[39m0.5\u001b[39m \u001b[39m+\u001b[39m p_constraint)\n\u001b[0;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     to_minimize \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39m-\u001b[39mac(x\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), gp\u001b[39m=\u001b[39;49mgp, y_max\u001b[39m=\u001b[39;49my_max)\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m x_try \u001b[39min\u001b[39;00m x_seeds:\n\u001b[0;32m     77\u001b[0m     \u001b[39m# Find the minimum of minus the acquisition function\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     res \u001b[39m=\u001b[39m minimize(\u001b[39mlambda\u001b[39;00m x: to_minimize(x),\n\u001b[0;32m     79\u001b[0m                    x_try,\n\u001b[0;32m     80\u001b[0m                    bounds\u001b[39m=\u001b[39mbounds,\n\u001b[0;32m     81\u001b[0m                    method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL-BFGS-B\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\util.py:148\u001b[0m, in \u001b[0;36mUtilityFunction.utility\u001b[1;34m(self, x, gp, y_max)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mutility\u001b[39m(\u001b[39mself\u001b[39m, x, gp, y_max):\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mucb\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ucb(x, gp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkappa)\n\u001b[0;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mei\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    150\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ei(x, gp, y_max, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxi)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bayes_opt\\util.py:158\u001b[0m, in \u001b[0;36mUtilityFunction._ucb\u001b[1;34m(x, gp, kappa)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m    157\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m     mean, std \u001b[39m=\u001b[39m gp\u001b[39m.\u001b[39;49mpredict(x, return_std\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    160\u001b[0m \u001b[39mreturn\u001b[39;00m mean \u001b[39m+\u001b[39m kappa \u001b[39m*\u001b[39m std\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:385\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.predict\u001b[1;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     dtype, ensure_2d \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, ensure_2d\u001b[39m=\u001b[39;49mensure_2d, dtype\u001b[39m=\u001b[39;49mdtype, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mX_train_\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# Unfitted;predict based on GP prior\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nGaussianProcessRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "BO_instance = BO()\n",
    "BO_instance.initializeOptimizer(lossFunction, param_bounds, loadingProgress=False)\n",
    "BO_instance.run()\n",
    "solution_dict, solution_tuple, best_solution_loss = BO_instance.outputResult()\n",
    "print(solution_dict)\n",
    "\n",
    "for param in solution_dict:\n",
    "    print(f\"{param}: {solution_dict[param]}\")\n",
    "\n",
    "print(f\"Best solution loss = {best_solution_loss:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can run this cell repeatedly to load the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New optimizer is now aware of 180 points.\n",
      "{'c1': 1428.1996230619955, 'c2': 1000.0, 'c3': 100.0}\n",
      "c1: 1428.1996230619955\n",
      "c2: 1000.0\n",
      "c3: 100.0\n",
      "Best solution loss = -24.8382\n"
     ]
    }
   ],
   "source": [
    "BO_instance = BO()\n",
    "BO_instance.initializeOptimizer(lossFunction, param_bounds, loadingProgress=True)\n",
    "BO_instance.run()\n",
    "solution_dict, solution_tuple, best_solution_loss = BO_instance.outputResult()\n",
    "print(solution_dict)\n",
    "\n",
    "for param in solution_dict:\n",
    "    print(f\"{param}: {solution_dict[param]}\")\n",
    "\n",
    "print(f\"Best solution loss = {best_solution_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
