{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c2a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.acquisition.multi_objective import qExpectedHypervolumeImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.multi_objective.box_decompositions import NondominatedPartitioning\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils import standardize\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130b0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramConfig = {\n",
    "    'c1': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1.0, 'name': 'W', 'unit': 'dimensionless', 'type': 'hardening'}, \n",
    "    'c2': {'lowerBound': 0, 'upperBound': 2, 'exponent': 1000.0, 'name': 'K', 'unit': 'MPa', 'type': 'yielding'}, \n",
    "    'c3': {'lowerBound': 0, 'upperBound': 1, 'exponent': 0.1, 'name': 'e0', 'unit': 'dimensionless', 'type': 'hardening'}, \n",
    "    'c4': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1.0, 'name': 'n', 'unit': 'dimensionless', 'type': 'hardening'}, \n",
    "    'c5': {'lowerBound': 0, 'upperBound': 2, 'exponent': 1000.0, 'name': 'sigma_y', 'unit': 'MPa', 'type': 'yielding'}, \n",
    "    'c6': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1000.0, 'name': 'sigma_sat', 'unit': 'MPa', 'type': 'hardening'}, \n",
    "    'c7': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1000.0, 'name': 'b', 'unit': 'dimensionless', 'type': 'hardening'}\n",
    "}\n",
    "\n",
    "geometries = ['NDBR50', 'NDBR6', 'CHD6']\n",
    "\n",
    "yieldingIndices = {'NDBR50': 200, 'NDBR6': 200, 'CHD6': 1200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094207a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'c1': 0.865545392036438, 'c2': 1796.846435546875, 'c3': 0.09999999403953552, 'c4': 0.06777941435575485, 'c5': 923.2322387695312, 'c6': 457.3926696777344, 'c7': 138.47036743164062}]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "combined_interpolated_params_to_geoms_FD_Curves_smooth = np.load('combined_interpolated_param_to_geom_FD_Curves_smooth.npy', allow_pickle=True).tolist()\n",
    "targetCurves = np.load('targetCurves.npy', allow_pickle=True).tolist()\n",
    "\n",
    "# Define the function for the RMSE loss\n",
    "def lossFD(targetDisp, targetForce, simForce):\n",
    "    return torch.sqrt(torch.mean((simForce - targetForce)**2))\n",
    "\n",
    "# Calculate losses and prepare data for model\n",
    "params = []\n",
    "losses = []\n",
    "\n",
    "for param_tuple, geom_to_simCurves in combined_interpolated_params_to_geoms_FD_Curves_smooth.items():\n",
    "    #print(param_tuple)\n",
    "    params.append([value for param, value in param_tuple])\n",
    "    # The minus sign is because BOTORCH tries to maximize objectives, but we want to minimize the loss\n",
    "    loss_iter = []\n",
    "    for geometry in geometries:\n",
    "        yieldingIndex = yieldingIndices[geometry]\n",
    "        loss_iter.append(- lossFD(\n",
    "            torch.tensor(targetCurves[geometry][\"displacement\"][yieldingIndex:]), \n",
    "            torch.tensor(targetCurves[geometry][\"force\"][yieldingIndex:]), \n",
    "            torch.tensor(geom_to_simCurves[geometry][\"force\"][yieldingIndex:])\n",
    "        ))\n",
    "    losses.append(loss_iter)\n",
    "    \n",
    "# Convert your data to the tensor(float 64)\n",
    "X = torch.tensor(params, dtype=torch.float64)\n",
    "Y = torch.stack([torch.tensor(loss, dtype=torch.float64) for loss in losses])\n",
    "\n",
    "# Normalize X to have range of [0, 1]\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_normalized = torch.tensor(minmax_scaler.fit_transform(X.numpy()), dtype=torch.float64)\n",
    "\n",
    "# Standardize Y to have zero mean and unit variance\n",
    "standard_scaler = StandardScaler()\n",
    "Y_standardized = torch.tensor(standard_scaler.fit_transform(Y.numpy()), dtype=torch.float64)\n",
    "\n",
    "# Define the bounds of the search space\n",
    "lower_bounds = torch.tensor([paramConfig[param]['lowerBound'] * paramConfig[param]['exponent'] for param in paramConfig.keys()]).float()\n",
    "upper_bounds = torch.tensor([paramConfig[param]['upperBound'] * paramConfig[param]['exponent'] for param in paramConfig.keys()]).float()\n",
    "\n",
    "\n",
    "lower_bounds = minmax_scaler.transform(lower_bounds.reshape(1, -1)).squeeze()\n",
    "upper_bounds = minmax_scaler.transform(upper_bounds.reshape(1, -1)).squeeze()\n",
    "\n",
    "bounds = torch.tensor(np.array([lower_bounds, upper_bounds])).float()\n",
    "\n",
    "# Initialize model\n",
    "model = SingleTaskGP(X_normalized, Y_standardized)\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "fit_gpytorch_model(mll)\n",
    "\n",
    "# **Reference Point**\n",
    "\n",
    "# qEHVI requires specifying a reference point, which is the lower bound on the objectives used for computing hypervolume. \n",
    "# In this tutorial, we assume the reference point is known. In practice the reference point can be set \n",
    "# 1) using domain knowledge to be slightly worse than the lower bound of objective values, \n",
    "# where the lower bound is the minimum acceptable value of interest for each objective, or \n",
    "# 2) using a dynamic reference point selection strategy.\n",
    "\n",
    "ref_point = Y_standardized.min(dim=0).values - 0.01\n",
    "\n",
    "partitioning = NondominatedPartitioning(ref_point=ref_point, Y=Y_standardized)\n",
    "acq_func = qExpectedHypervolumeImprovement(\n",
    "    model=model,\n",
    "    partitioning=partitioning,\n",
    "    ref_point=ref_point,\n",
    "    objective=IdentityMCMultiOutputObjective(),\n",
    ")\n",
    "\n",
    "\n",
    "# Optimize the acquisition function\n",
    "candidates, _ = optimize_acqf(\n",
    "    acq_function=acq_func,\n",
    "    bounds=bounds,\n",
    "    q=1, #q: This is the number of points to sample in each step. \n",
    "    num_restarts=10, # num_restarts: This is the number of starting points for the optimization.\n",
    "    raw_samples=1000, # raw_samples: This is the number of samples to draw when initializing the optimization\n",
    ")\n",
    "\n",
    "# Unnormalize the candidates\n",
    "candidates = minmax_scaler.inverse_transform(candidates.detach().numpy())\n",
    "\n",
    "#converting to dictionary\n",
    "next_param_dicts = [{param: value.item() for param, value in zip(paramConfig.keys(), next_param)} for next_param in candidates]\n",
    "\n",
    "print(next_param_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
